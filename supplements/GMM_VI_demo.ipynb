{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677d05f2",
   "metadata": {},
   "source": [
    "# The Data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4acb90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='eruptions', ylabel='waiting'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_csv(\"https://gist.githubusercontent.com/curran/4b59d1046d9e66f2787780ad51a1cd87/raw/9ec906b78a98cf300947a37b56cfe70d01183200/data.tsv\", sep = \"\\t\")\n",
    "data.head()\n",
    "data.plot.scatter(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3013911",
   "metadata": {},
   "source": [
    "\n",
    "- observed data: $y_1, \\ldots, y_n$ (bivariate)\n",
    "- $y_i = (y_i^1, y_i^2) = (\\text{eruption}^i, \\text{waiting}^i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826674a9",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed017e32",
   "metadata": {},
   "source": [
    "### Gaussian mixtures likelihood\n",
    "\n",
    "\n",
    "- $x_i$: binary...whether it came from first normal distribution, or second\n",
    "- $\\theta = (\\mu_0, \\Lambda_0, \\mu_1, \\Lambda_1, \\pi)$ mixture weight and parameters of all normal distributions\n",
    "\n",
    "\n",
    "$$\n",
    "p(y_i \\mid x_i = 0, \\theta) = \\text{Normal}\n",
    "\\left(\n",
    "\\mu_0, \n",
    "\\Sigma_0\n",
    "\\right)\n",
    "= \\text{Normal}\n",
    "\\left(\n",
    "\\mu_0, \n",
    "\\Lambda_0^{-1}\n",
    "\\right)\n",
    "$$\n",
    "$$\n",
    "p(y_i \\mid x_i = 1, \\theta) = \\text{Normal}\n",
    "\\left(\n",
    "\\mu_1, \n",
    "\\Sigma_1\n",
    "\\right)\n",
    "= \\text{Normal}\n",
    "\\left(\n",
    "\\mu_1, \n",
    "\\Lambda_1^{-1}\n",
    "\\right)\n",
    "$$\n",
    "$$\n",
    "x_i \\mid \\pi \\sim \\text{Bernoulli}(\\pi)\n",
    "$$\n",
    "\n",
    "here $\\mu_0$ and $\\mu_1$ are $2 \\times 1$ vectors, and $\\Sigma_0$ and $\\Sigma_1$ are $2 \\times 2$ covariance matrices. $\\Lambda_i$ are both precision matrices. $0 < \\pi < 1$. \n",
    "\n",
    "We assume all data points are exchangeable/iid:\n",
    "\n",
    "$p(\\mathbf{y} \\mid \\mathbf{x}, \\theta) := \\prod_{i=1}^n p(y_i \\mid x_i, \\theta)$ \n",
    "\n",
    "$p(\\mathbf{x} \\mid \\theta) = \\prod_{i=1}^n p(x_i \\mid \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3519d22",
   "metadata": {},
   "source": [
    "### The Prior\n",
    "\n",
    "We'll use precision matrices throughout the rest of this demo. For $i=0,1$:\n",
    "\n",
    "\n",
    "- $\\mu_i\\mid \\Lambda_i \\sim \\text{Normal}(\\mu^*, [\\beta^* \\Lambda_i]^{-1})$, \n",
    "- $\\Lambda_i \\sim \\text{Wishart}(\\mathbf{W}, \\nu)$\n",
    "- $\\pi \\sim \\text{Beta}(a,b)$\n",
    "\n",
    "we'll package up the prior hyperparameters in the following order: $(\\mu^*, \\beta^*, \\mathbf{W}, \\nu,a,b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa03da3",
   "metadata": {},
   "source": [
    "# A Variational Approximation of $p(\\theta, \\mathbf{X} \\mid \\mathbf{y})$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450106a",
   "metadata": {},
   "source": [
    "## Initializing our Approximation \n",
    "Let's pick our approximating class of distributions:\n",
    "\n",
    "\\begin{align*}\n",
    "g(\\mathbf{x}, \\theta) \n",
    "&= g(\\mathbf{x}, \\pi, \\mu_0, \\mu_1, \\Lambda_0, \\Lambda_1) \\\\\n",
    "&= g(\\mathbf{x})g(\\pi)g(\\mu_0, \\Lambda_0)g(\\mu_1, \\Lambda_1) \\\\\n",
    "&= g(\\mathbf{x})g(\\pi)g(\\mu_0, \\Lambda_0)g(\\mu_1, \\Lambda_1)\n",
    "\\end{align*}\n",
    "\n",
    "Specifically:\n",
    "\n",
    "- $g(\\mathbf{x}) = \\prod_{i=1}^n \\text{Bernoulli}(p_i)$\n",
    "- $g(\\pi) = \\text{Beta}(\\alpha, \\beta)$\n",
    "- $g(\\mu_0, \\Lambda_0) = g(\\mu_0 \\mid \\Lambda_0)g(\\Lambda_0) = \\text{Normal}(m_0, [c_0 \\Lambda_0]^{-1})\\text{Wishart}(\\mathbf{H}_0, \\tau_0)$\n",
    "- $g(\\mu_1, \\Lambda_1) = g(\\mu_1 \\mid \\Lambda_1)g(\\Lambda_1) = \\text{Normal}(m_1, [c_1 \\Lambda_1]^{-1})\\text{Wishart}(\\mathbf{H}_1, \\tau_1)$\n",
    "\n",
    "We want to keep changing the \"hyperparameters\" $\\{p_i\\}_i, \\alpha, \\beta, m_0, m_1, c_0, c_1, \\mathbf{H}_0, \\mathbf{H}_1, \\tau_0, \\tau_1$ until they make a distribution that is very close to our posterior. At every iteration of our algorithm, we'll make changes to these hyperparameters, which is the same as making changes to $g$ distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed3710",
   "metadata": {},
   "source": [
    "## The ELBO\n",
    "\n",
    "After applying Bayes' rule, our target is:\n",
    "$$\n",
    "p(\\theta, \\mathbf{x} \\mid \\mathbf{y}) \\propto\n",
    "p(\\mathbf{y} \\mid \\mathbf{x}, \\theta) p(\\mathbf{x} \\mid \\theta) p(\\theta)\n",
    "$$\n",
    "and after taking the log on both sides:\n",
    "\\begin{align*}\n",
    "\\log p(\\theta, \\mathbf{x} \\mid \\mathbf{y}) \n",
    "&= \\log p(\\mathbf{y} \\mid \\mathbf{x}, \\theta) + \\log p(\\mathbf{x} \\mid \\theta) + \\log p(\\theta) + \\text{constant} \\\\\n",
    "&= \\sum_{i=1}^n \\left[ \\log p(y_i \\mid x_i, \\theta) + \\log p(x_i \\mid \\theta)\\right] + \\log p(\\theta) + c \\\\\n",
    "&= \\sum_{i=1}^n \\sum_{j=0}^1  \\mathbb{I}(x_i = j) \\left[ \\log p(y_i \\mid x_i=j, \\theta) + \\log p(x_i=j \\mid \\theta)\\right] + \\log p(\\theta) + c\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca2c53",
   "metadata": {},
   "source": [
    "The above quantity is not easy to take expectations of because we don't know $c$. But $c$ doesn't have anything to do with the parameters, so we'll drop it. In other words, this algorithm will take expectations of the following (i.e. the same quantity as the EM algorithm): \n",
    "\n",
    "\\begin{align*}\n",
    "\\log p(\\mathbf{y}, \\mathbf{x}, \\theta ) \n",
    "&= \\sum_{i=1}^n \\sum_{j=0}^1  \\mathbb{I}(x_i = j) \\left[ \\log p(y_i \\mid x_i=j, \\theta) + \\log p(x_i=j \\mid \\theta)\\right] + \\log p(\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Recall that the expectation of this is sometimes referred to as the ELBO.You can maximize the ELBO, or you can, like us, minimize the negative ELBO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1c9af",
   "metadata": {},
   "source": [
    "## Coordinate Ascent Variational Inference (CAVI)\n",
    "\n",
    "The VI algorithm will alternative between each $g$ factor, and update individual components of the approximating distribution. Each update formula must be derived, and it generally requires a bunch of math. We'll go through and prove each formula, but feel free to skip all the mathematical details on a first reading. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb9fcb",
   "metadata": {},
   "source": [
    "### 1. changing $\\mathbf{x}$'s distribution at each iteration\n",
    "\n",
    "Every iteration we will cycle through all of our $g$ distributions, and make changes to each one. We'll start off by describing how we make changes to $\\mathbf{x}$. This is the formula:\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{p}(\\mathbf{x}) \n",
    "&= \\exp[\\mathbb{E}_{-\\mathbf{x}}\\{\\log p(\\mathbf{y}, \\mathbf{x}, \\theta ) \\} ] \\\\\n",
    "&= \\prod_{i=1}^n \\tilde{p}(x_i)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Here we take the expectation with respect to \n",
    "$$\n",
    "g(\\pi)g(\\mu_0 \\mid \\Lambda_0)g(\\mu_1 \\mid \\Lambda_1)g(\\Lambda_0)g(\\Lambda_1).\n",
    "$$ \n",
    "\n",
    "In other words, we take the expectation with respect to the approximate distribution of everything except $\\mathbf{x}$\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{-\\mathbf{x}}\\{\\log p(\\mathbf{y}, \\mathbf{x}, \\theta ) \\}\n",
    "&=\n",
    "\\sum_{i=1}^n  \n",
    "\\mathbb{E}_{-\\mathbf{x}} \\left[\\log p(y_i \\mid x_i, \\theta)\\right] \n",
    "+ \\mathbb{E}_{-\\mathbf{x}}\\left[\\log p(x_i \\mid \\theta)\\right]\n",
    "\\end{align*}\n",
    "\n",
    "We can focus on one $i$ at a time, because the exponential of the above will factor. For each $i$, we're taking the expectation of two pieces:\n",
    "\n",
    "$$\n",
    "\\log p(y_i \\mid x_i, \\theta) =\n",
    "-\\log(2\\pi) + .5 \\log \\det \\Lambda_{x_i} - .5 (y_i - \\mu_{x_i})^\\intercal \\Lambda_{x_i}(y_i - \\mu_{x_i})\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "\\log p(x_i  \\mid \\theta) = x_i \\log \\pi + (1-x_i)\\log(1-\\pi) \n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c936f4fa",
   "metadata": {},
   "source": [
    "Taking the expectation of the first piece is a little more difficult, but there are some formulas from Wiki (e.g. [1](https://en.wikipedia.org/wiki/Wishart_distribution#Log-expectation) and [2](https://en.wikipedia.org/wiki/Multivariate_gamma_function) and [3](https://en.wikipedia.org/wiki/Quadratic_form_(statistics)#Expectation) and [4](https://en.wikipedia.org/wiki/Law_of_total_expectation) and [5](https://en.wikipedia.org/wiki/Trace_(linear_algebra)). ) we can take advtange of. Remember that any summand that's free of $\\mathbf{x}$ can be disregarded as an additive constant that will be taken care of later when we figure out the normalizing constant.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{-\\mathbf{x}}[\n",
    "\\log p(y_i \\mid x_i, \\theta)] \n",
    "&=\n",
    "-\\log(2\\pi) + .5 \\mathbb{E}_{-\\mathbf{x}}[  \\log \\det \\Lambda_{x_i}] \n",
    "- .5 \\mathbb{E}_{-\\mathbf{x}}[ (y_i - \\mu_{x_i})^\\intercal \\Lambda_{x_i}(y_i - \\mu_{x_i})] ] \\\\\n",
    "&=\n",
    "c + .5 \\left\\{ \\psi_2(\\tau_{x_i}/2)  + 2 \\log (2) + \\log \\det \\mathbf{H}_{x_i} \\right\\} \n",
    "- .5 \\mathbb{E}_{-\\mathbf{x}}[ (y_i - \\mu_{x_i})^\\intercal \\Lambda_{x_i}(y_i - \\mu_{x_i})] ] \\\\\n",
    "&=\n",
    "c + .5 \\left\\{ \n",
    "\\psi(\\tau_{x_i}/2 ) + \\psi((\\tau_{x_i}-1)/2  )\\right\\}\n",
    "- .5 \\mathbb{E}_{-\\mathbf{x}}[ (y_i - \\mu_{x_i})^\\intercal \\Lambda_{x_i}(y_i - \\mu_{x_i})] ]\\\\\n",
    "&=\n",
    "c + .5 \\left\\{ \n",
    "\\psi(\\tau_{x_i}/2 ) + \\psi((\\tau_{x_i}-1)/2  )\\right\\}\n",
    "- .5 \\mathbb{E}_{\\Lambda} \\left\\{ \\mathbb{E}_{\\mu \\mid \\Lambda}[ (y_i - \\mu_{x_i})^\\intercal \\Lambda_{x_i}(y_i - \\mu_{x_i})] ]\\right\\} \\\\\n",
    "&=\n",
    "c + .5 \\left\\{ \n",
    "\\psi(\\tau_{x_i}/2 ) + \\psi((\\tau_{x_i}-1)/2  )\\right\\}\n",
    "- .5 \\mathbb{E}_{\\Lambda} \\left\\{ \\text{tr}(\\Lambda_{x_i}\\Lambda_{x_i}^{-1} )/c_{x_i} + (m_{x_i} - y_i)^\\intercal\\Lambda_{x_i}(m_{x_i} - y_i) \\right\\} \\\\\n",
    "&=\n",
    "c + .5 \\left\\{ \n",
    "\\psi(\\tau_{x_i}/2 ) + \\psi((\\tau_{x_i}-1)/2  )\\right\\}\n",
    "- .5 \\mathbb{E}_{\\Lambda} \\left\\{ 2/c_{x_i} + (m_{x_i} - y_i)^\\intercal\\Lambda_{x_i}(m_{x_i} - y_i) \\right\\} \\\\\n",
    "&=\n",
    "c + .5 \\left\\{ \n",
    "\\psi(\\tau_{x_i}/2 ) + \\psi((\\tau_{x_i}-1)/2  )\\right\\}\n",
    "- .5  \\left\\{ 2/c_{x_i} + (m_{x_i} - y_i)^\\intercal \\mathbb{E}_{\\Lambda}[ \\Lambda_{x_i}](m_{x_i} - y_i) \\right\\} \\\\\n",
    "&=\n",
    "c + .5 \\left\\{ \n",
    "\\psi(\\tau_{x_i}/2 ) + \\psi((\\tau_{x_i}-1)/2  )\\right\\}\n",
    "- .5  \\left\\{ 2/c_{x_i} + (m_{x_i} - y_i)^\\intercal \\mathbf{H}_{x_i} \\tau_{x_i} (m_{x_i} - y_i) \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c716e4",
   "metadata": {},
   "source": [
    "\n",
    "Taking the expectation of the second piece also requires some [math-stats formulae](https://en.wikipedia.org/wiki/Beta_distribution#Moments_of_logarithmically_transformed_random_variables):\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{-\\mathbf{x}}[ \\log p(x_i  \\mid \\theta)] \n",
    "&= x_i \\mathbb{E}_{-\\mathbf{x}} \\log \\pi + (1-x_i)\\mathbb{E}_{-\\mathbf{x}}\\log(1-\\pi) \\\\\n",
    "&= x_i [\\psi(\\alpha) - \\psi(\\alpha + \\beta)] + (1-x_i)[\\psi(\\beta) - \\psi(\\alpha + \\beta)] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Both of these functions involve the \"digamma function\" which is [available in Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.digamma.html). The $\\psi_2$ with a subscript in it is multivariate digamma which is a bit different.\n",
    "\n",
    "\n",
    "<!-- Putting it all together:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\mathbb{E}_{-\\mathbf{x}}\\{\\log p(\\mathbf{y}, \\mathbf{x}, \\theta ) \\}\\\\\n",
    "&= \n",
    "\\sum_{i=1}^n \n",
    "-\\log(2\\pi) + .5 \\left\\{ \n",
    "\\psi(\\tau_{x_i}/2 ) + \\psi((\\tau_{x_i}-1)/2  )\n",
    "+ 2 \\log (2) + \\log \\det \\mathbf{H}_{x_i} \\right\\} \n",
    "- .5 \\left\\{ (y_i - m_{x_i})^\\intercal ] \\tau_{x_i} \\mathbf{H}_{x_i} (y_i - m_{x_i})   \\right\\}\n",
    "+ x_i [\\psi(\\alpha) - \\psi(\\alpha + \\beta)] + (1-x_i)[\\psi(\\beta) - \\psi(\\alpha + \\beta)] \\\\\n",
    "&= c +\n",
    "\\sum_{i=1}^n \n",
    ".5 \\log \\det \\mathbf{H}_{x_i} \n",
    "- .5 \\left\\{ (y_i - m_{x_i})^\\intercal ] \\tau_{x_i} \\mathbf{H}_{x_i} (y_i - m_{x_i})   \\right\\}\n",
    "+ x_i [\\psi(\\alpha) - \\psi(\\alpha + \\beta)] + (1-x_i)[\\psi(\\beta) - \\psi(\\alpha + \\beta)]\n",
    "+ .5 \\psi(\\tau_{x_i}/2 ) + .5\\psi((\\tau_{x_i}-1)/2  )\n",
    "\\end{align*}\n",
    " -->\n",
    "All that's left is that you have to exponentiate this, and then normalize. Normalization is easy because we can just divide by the sum of the unnormalized probabilities. \n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{p}(\\mathbf{x}) \n",
    "&= \\exp[\\mathbb{E}_{-\\mathbf{x}}\\{\\log p(\\mathbf{y}, \\mathbf{x}, \\theta ) \\} ] \\\\\n",
    "&= \\prod_{i=1}^n \\tilde{p}(x_i)\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a47c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import digamma\n",
    "\n",
    "def update_x(old_g_hyper_params, ydata, prior):\n",
    "    \"\"\"this function takes and old g approx posterior\n",
    "    and then returns a new g approx posterior\n",
    "    the new one will have updated p probabilities\n",
    "    NB: we only keep track of the probability that \n",
    "    xi is 1. but in order to get this, you need to \n",
    "    compute both and then normalize\"\"\"\n",
    "    # unpackage g and prior hyper parameters\n",
    "    mu_star, beta_star, W, nu, a, b = prior\n",
    "    old_p_array, alpha, beta, m_zero, m_one, c_zero, c_one, H_zero, H_one, tau_zero, tau_one = old_g_hyper_params\n",
    "\n",
    "    new_p_array = np.empty(old_p_array.shape)\n",
    "    tmp = np.empty((2,))\n",
    "    row = 0\n",
    "    \n",
    "    for idx,yi in ydata.iterrows():\n",
    "        # compute log of unnormalized probabilities\n",
    "        first_log_unnorm  = .5*(digamma(.5*tau_zero) + digamma(.5*(tau_zero-1)))\n",
    "        first_log_unnorm -= 1/c_zero \n",
    "        first_log_unnorm -= .5*tau_zero * (np.transpose(m_zero - yi.values) @ H_zero @ (m_zero - yi.values))\n",
    "        first_log_unnorm += digamma(alpha) - digamma(alpha + beta)\n",
    "        \n",
    "        secnd_log_unnorm  = .5*(digamma(.5*tau_one) + digamma(.5*(tau_one-1)))\n",
    "        secnd_log_unnorm -= 1/c_one \n",
    "        secnd_log_unnorm -= .5*tau_one * (np.transpose(m_one - yi.values) @ H_one @ (m_one - yi.values))\n",
    "        secnd_log_unnorm += digamma(beta) - digamma(alpha + beta)\n",
    "        \n",
    "        # now convert log unnorm probabilities into probabilities\n",
    "        m = np.max([first_log_unnorm, secnd_log_unnorm])\n",
    "        first_log_unnorm -= m\n",
    "        secnd_log_unnorm -= m\n",
    "        tmp[0] = np.exp(first_log_unnorm)\n",
    "        tmp[1] = np.exp(secnd_log_unnorm)\n",
    "        tmp = tmp/np.sum(tmp)\n",
    "        \n",
    "        # store the probability that xi is 1 (ignore the other becuase they sum to 1)\n",
    "        new_p_array[row] = tmp[1]\n",
    "        row += 1\n",
    "    return (new_p_array, alpha, beta, m_zero, m_one, c_zero, c_one, H_zero, H_one, tau_zero, tau_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5381a5b3",
   "metadata": {},
   "source": [
    "### 2. changing $\\pi$'s distribution at each iteration\n",
    "\n",
    "Here we take the expectation of \n",
    "\n",
    "\\begin{align*}\n",
    "\\log p(\\mathbf{y}, \\mathbf{x}, \\theta ) \n",
    "&= \\sum_{i=1}^n  \\left[ \\log p(y_i \\mid x_i=j, \\theta) + \\log p(x_i=j \\mid \\theta)\\right] + \\log p(\\theta)\\\\\n",
    "&= c + \\sum_{i=1}^n   x_i \\log \\pi + (1-x_i)\\log(1-\\pi) + \\log p(\\theta) \\\\\n",
    "&= c + \\sum_{i=1}^n   x_i \\log \\pi + (1-x_i)\\log(1-\\pi) + \\log [\\pi^{a-1}(1-\\pi)^{b-1}] \\\\\n",
    "&= c + \\sum_{i=1}^n   x_i \\log \\pi + (1-x_i)\\log(1-\\pi) + (a-1)\\log \\pi + (b-1)\\log(1-\\pi) \\\\\n",
    "&= (\\sum_i x_i + a - 1)\\log \\pi + (n - \\sum_i x_i + b - 1)\\log(1-\\pi)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with respect to \n",
    "$$\n",
    "g(\\mathbf{x})g(\\mu_0 \\mid \\Lambda_0)g(\\mu_1 \\mid \\Lambda_1)g(\\Lambda_0)g(\\Lambda_1).\n",
    "$$ \n",
    "\n",
    "This is easier than updating the other components.\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{p}(\\pi) \n",
    "&= \\exp[\\mathbb{E}_{-\\pi}\\{\\log p(\\mathbf{y}, \\mathbf{x}, \\theta ) \\} ] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "and focusing on the exponent:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{-\\pi}\\{\\log p(\\mathbf{y}, \\mathbf{x}, \\theta ) \\}\n",
    "&= \\mathbb{E}_{-\\pi}(\\sum_i x_i + a - 1) \\log \\pi + \\mathbb{E}_{-\\pi}(n - \\sum_i x_i + b - 1)\\log(1-\\pi)\\\\\n",
    "&= (\\sum_i p_i + a - 1) \\log \\pi + (n - \\sum_i p_i + b - 1)\\log(1-\\pi)\n",
    "\\end{align*}\n",
    "\n",
    "and we end up with a Beta distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453fe2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pi(old_g_hyper_params, ydata, prior_hyperparams):\n",
    "    \"\"\"this function takes and old g approx posterior\n",
    "    and then returns a new g approx posterior\n",
    "    the new one will have updated alpha and betas\"\"\"\n",
    "    mu_star, beta_star, W, nu, a, b = prior_hyperparams\n",
    "    p_array, old_alpha, old_beta, m_zero, m_one, c_zero, c_one, H_zero, H_one, tau_zero, tau_one = old_g_hyper_params\n",
    "    sum_ps = np.sum(p_array)   \n",
    "    return (p_array, sum_ps + a, ydata.shape[0] - sum_ps + b, m_zero, m_one, c_zero, c_one, H_zero, H_one, tau_zero, tau_one)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da94fd9",
   "metadata": {},
   "source": [
    "### 3. changing $\\mu_0$ and $\\Lambda_0$'s distribution at each iteration\n",
    "\n",
    "\n",
    "Here we take the expectation of \n",
    "\n",
    "\\begin{align*}\n",
    "\\log p(\\mathbf{y}, \\mathbf{x}, \\theta ) \n",
    "&= \\sum_{i=1}^n  \\sum_{j=0}^2 \\mathbb{I}(x_i = j)\\left[ \\log p(y_i \\mid x_i=j, \\theta) + \\log p(x_i=j \\mid \\theta)\\right]  + \\log p(\\theta)\\\\\n",
    "&= c + \\sum_{i=1}^n  \\mathbb{I}(x_i = 0)\\left[ \\log p(y_i \\mid x_i=0, \\theta) + \\log p(x_i=0 \\mid \\theta)\\right]  + \\log p(\\mu_0 \\mid \\Lambda_0) + \\log p(\\Lambda_0) \\\\\n",
    "&= c + \\sum_{i=1}^n  \\mathbb{I}(x_i = 0)\\log p(y_i \\mid x_i=0, \\theta)  + \\log p(\\mu_0 \\mid \\Lambda_0) + \\log p(\\Lambda_0)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "with respect to \n",
    "$$\n",
    "g(\\mathbf{x})g(\\pi)g(\\mu_1 \\mid \\Lambda_1)g(\\Lambda_1).\n",
    "$$ \n",
    "\n",
    "Most of the things we're integrating with respect to aren't present, so it's relatively simple to take the expectation:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\mathbb{E}_{-(\\mu_0, \\Lambda_0)}\n",
    "\\left[\n",
    "\\sum_{i=1}^n  \\mathbb{I}(x_i = 0)\\log p(y_i \\mid x_i=0, \\theta) \n",
    "+\\log p(\\mu_0 \\mid \\Lambda_0) + \\log p(\\Lambda_0)\n",
    "\\right] \\\\\n",
    "&= \n",
    "\\mathbb{E}_{-(\\mu_0, \\Lambda_0)}\n",
    "\\left[\n",
    "\\sum_{i=1}^n  \\mathbb{I}(x_i = 0)\\left[ \\log p(y_i \\mid x_i=0, \\theta) \\right]\n",
    "\\right] \n",
    "+\\log p(\\mu_0 \\mid \\Lambda_0) + \\log p(\\Lambda_0) + c\n",
    "\\\\\n",
    "&\\mathbb{E}_{-(\\mu_0, \\Lambda_0)}\n",
    "\\left[\n",
    "\\sum_{i=1}^n  \\mathbb{I}(x_i = 0)\\left[  .5 \\log \\det \\Lambda_{0} - .5 (y_i - \\mu_{0})^\\intercal \\Lambda_{0}(y_i - \\mu_{0})]  \\right] \\right]  \n",
    "+\\log p(\\mu_0 \\mid \\Lambda_0) + \\log p(\\Lambda_0)+ c\\\\\n",
    "&= \n",
    "\\sum_{i=1}^n  (1-p_i) \\left[  .5 \\log \\det \\Lambda_{0} - .5 (y_i - \\mu_{0})^\\intercal \\Lambda_{0}(y_i - \\mu_{0})]  \\right] \n",
    "+\\log p(\\mu_0 \\mid \\Lambda_0) + \\log p(\\Lambda_0)+ c\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62614b3",
   "metadata": {},
   "source": [
    "\n",
    "The more difficult part is rearranging this into the appropriate Normal-Wishart distribution. Let's write it all out at first and then we can group terms together. Again we make use of some properties of the [trace operator](https://en.wikipedia.org/wiki/Trace_(linear_algebra)).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "&\\sum_{i=1}^n  (1-p_i) \\left[  .5 \\log \\det \\Lambda_{0} - .5 (y_i - \\mu_{0})^\\intercal \\Lambda_{0}(y_i - \\mu_{0})]  \\right] + .5 \\log \\det \\Lambda_0 - .5 \\beta^*(\\mu_0 - \\mu^*)^\\intercal\\Lambda_0(\\mu_0 - \\mu^*) \\\\\n",
    "&\\hspace{10mm} + \n",
    ".5*(\\nu-2-1)\\log \\det \\Lambda_0 - .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_0)\n",
    "+ c \\\\\n",
    "&= .5 \\log \\det \\Lambda_{0} \n",
    "\\left\\{ \\sum_{i=1}^n  (1-p_i) + 1 + \\nu-2-1 \\right\\}\n",
    "- .5 \\sum_{i=1}^n  (1-p_i) (y_i - \\mu_{0})^\\intercal \\Lambda_{0}(y_i - \\mu_{0})   \\\\\n",
    "&\\hspace{10mm} \n",
    "- .5 \\beta^*(\\mu_0 - \\mu^*)^\\intercal\\Lambda_0(\\mu_0 - \\mu^*) - .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_0)\n",
    "+ c \\\\\n",
    "&= .5 \\log \\det \\Lambda_{0} \n",
    "\\left\\{ \\sum_{i=1}^n  (1-p_i) + 1 + \\nu-2-1 \\right\\}\n",
    "- .5 \\sum_{i=1}^n  (1-p_i) (\\mu_{0} - y_i )^\\intercal \\Lambda_{0}(\\mu_{0}- y_i )   \\\\\n",
    "&\\hspace{10mm} \n",
    "- .5 \\beta^*(\\mu_0 - \\mu^*)^\\intercal\\Lambda_0(\\mu_0 - \\mu^*) - .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_0)\n",
    "+ c \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The second chunk is tedious to simplify\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n  (1-p_i) (\\mu_{0} - y_i )^\\intercal \\Lambda_{0}(\\mu_{0}- y_i ) \n",
    "&=\n",
    "\\sum_{i=1}^n  (1-p_i) \\left[ (\\mu_{0}^\\intercal \\Lambda_{0}\\mu_{0} - 2 \\mu_0^\\intercal \\Lambda_0 y_i + y_i^\\intercal \\Lambda_0 y_i \\right] \\\\\n",
    "&=\n",
    "\\sum_{i=1}^n  (1-p_i) \\mu_{0}^\\intercal \\Lambda_{0}\\mu_{0} - 2 \\sum_{i=1}^n  (1-p_i)\\mu_0^\\intercal \\Lambda_0 y_i + \\sum_{i=1}^n  (1-p_i)y_i^\\intercal \\Lambda_0 y_i  \\\\\n",
    "&=\n",
    "\\mu_{0}^\\intercal s_n \\Lambda_{0}\\mu_{0} - 2 \\mu_0^\\intercal s_n \\Lambda_0\\sum_{i=1}^n  (1-p_i) \\frac{y_i}{s_n} + \\sum_{i=1}^n  (1-p_i)y_i^\\intercal \\Lambda_0 y_i \\\\\n",
    "&=\n",
    "\\mu_{0}^\\intercal s_n \\Lambda_{0}\\mu_{0} \n",
    "- 2 \\mu_0^\\intercal s_n \\Lambda_0\\ \\tilde{y} \n",
    "+\\sum_{i=1}^n  (1-p_i)y_i^\\intercal \\Lambda_0 y_i\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where $s_n = \\sum_{i=1}^n  (1-p_i) $ and $\\tilde{y} = s_n^{-1}\\sum_i (1-p_i) y_i$ is a weighted average of the observations.\n",
    "\n",
    "\n",
    "\n",
    "The third chunk is not as bad:\n",
    "\n",
    "\\begin{align*}\n",
    "& \\beta^*(\\mu_0 - \\mu^*)^\\intercal\\Lambda_0(\\mu_0 - \\mu^*) \\\\\n",
    "&= \\beta^*\\mu_0^\\intercal \\Lambda_0 \\mu_0 - 2 \\beta^*\\mu_0^\\intercal\\Lambda_0 \\mu^* + \\beta^*\\mu^* \\Lambda_0 \\mu^* \n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a8a94",
   "metadata": {},
   "source": [
    "Let's get the final formula now\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "&\\mathbb{E}_{-(\\mu_0, \\Lambda_0)}\n",
    "\\log p(\\mathbf{y}, \\mathbf{x}, \\theta ) \n",
    "\\\\\n",
    "&=.5 \\log \\det \\Lambda_{0} \n",
    "\\left\\{ \\sum_{i=1}^n  (1-p_i) + 1 + \\nu-2-1 \\right\\}\n",
    "- .5 \\left\\{ \\mu_{0}^\\intercal s_n \\Lambda_{0}\\mu_{0} \n",
    "- 2 \\mu_0^\\intercal s_n \\Lambda_0\\ \\tilde{y} \n",
    "+\\sum_{i=1}^n  (1-p_i)y_i^\\intercal \\Lambda_0 y_i\\right\\}   \\\\\n",
    "&\\hspace{10mm} \n",
    "- .5 \\left\\{ \\beta^*\\mu_0^\\intercal \\Lambda_0 \\mu_0 - 2 \\beta^*\\mu_0^\\intercal\\Lambda_0 \\mu^* + \\beta^*\\mu^* \\Lambda_0 \\mu^* \\right\\}\n",
    "- .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_0)\n",
    "+ c \\\\ \\\\\n",
    "&=.5 \\log \\det \\Lambda_{0} \n",
    "\\left\\{ \\sum_{i=1}^n  (1-p_i) + 1 + \\nu-2-1 \\right\\}\n",
    "- .5 \\left\\{ \\mu_{0}^\\intercal s_n \\Lambda_{0}\\mu_{0} \n",
    "- 2 \\mu_0^\\intercal s_n \\Lambda_0\\ \\tilde{y} \n",
    "+\\sum_{i=1}^n  (1-p_i)y_i^\\intercal \\Lambda_0 y_i \n",
    "+\\beta^*\\mu_0^\\intercal \\Lambda_0 \\mu_0 - 2 \\beta^*\\mu_0^\\intercal\\Lambda_0 \\mu^* + \\beta^*\\mu^* \\Lambda_0 \\mu^* \\right\\} \\\\\n",
    "&\\hspace{10mm} \n",
    "- .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_0)\n",
    "+ c \\\\ \n",
    "&=.5 \\log \\det \\Lambda_{0} \n",
    "\\left\\{ \\sum_{i=1}^n  (1-p_i) + 1 + \\nu-2-1 \\right\\}\n",
    "- .5 \n",
    "\\left\\{ \n",
    "\\mu_{0}^\\intercal \\Lambda_{0}[s_n + \\beta^*]\\mu_{0} \n",
    "- 2 \\mu_0^\\intercal\\left[s_n \\Lambda_0\\ \\tilde{y}  +  \\beta^* \\Lambda_0 \\mu^*\\right]\n",
    "+\\sum_{i=1}^n  (1-p_i)y_i^\\intercal \\Lambda_0 y_i \n",
    "+ \\beta^*\\mu^* \\Lambda_0 \\mu^* \\right\\} \\\\\n",
    "&\\hspace{10mm} \n",
    "- .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_0)\n",
    "+ c \\\\\n",
    "&=.5 \\log \\det \\Lambda_{0} \n",
    "\\left\\{ \\sum_{i=1}^n  (1-p_i) + 1 + \\nu-2-1 \\right\\}\n",
    "- .5 \n",
    "\\left\\{ \n",
    "\\mu_{0}^\\intercal \\Lambda_{0}[s_n + \\beta^*]\\mu_{0} \n",
    "- 2 \\mu_0^\\intercal\\left[s_n \\Lambda_0\\ \\tilde{y}  +  \\beta^* \\Lambda_0 \\mu^*\\right]\n",
    "\\right\\} \\\\\n",
    "&\\hspace{10mm} \n",
    "-.5 \\left\\{ \n",
    "\\sum_{i=1}^n  (1-p_i)y_i^\\intercal \\Lambda_0 y_i \n",
    "+ \\beta^*\\mu^* \\Lambda_0 \\mu^* \\right\\} \n",
    "- .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_0)\n",
    "+ c \\\\\n",
    "&=.5 \\log \\det \\Lambda_{0} \n",
    "\\left\\{ \\sum_{i=1}^n  (1-p_i) + 1 + \\nu-2-1 \\right\\}\n",
    "- .5 \n",
    "\\left(\\mu_{0} -  \\left[\\Lambda_{0}[s_n + \\beta^*] \\right]^{-1} \\left[s_n \\Lambda_0\\ \\tilde{y}  +  \\beta^* \\Lambda_0 \\mu^*\\right] \\right)^\\intercal\n",
    "\\left[\\Lambda_{0}[s_n + \\beta^*] \\right]\n",
    "\\left(\\mu_{0} -  \\left[\\Lambda_{0}[s_n + \\beta^*] \\right]^{-1} \\left[s_n \\Lambda_0\\ \\tilde{y}  +  \\beta^* \\Lambda_0 \\mu^*\\right] \\right)\n",
    " \\\\\n",
    "&\\hspace{10mm} \n",
    "-.5 \\left\\{ \n",
    "\\sum_{i=1}^n  (1-p_i)y_i^\\intercal \\Lambda_0 y_i \n",
    "+ \\beta^*\\mu^* \\Lambda_0 \\mu^* \\right\\} \n",
    "- .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_0)\n",
    "+ c \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f9c69",
   "metadata": {},
   "source": [
    "We recognize the quadratic form in the middle, which means $g(\\mu_0 \\mid \\Lambda_0)$ can be updated to a normal distribution with mean \n",
    "$$\n",
    "\\tilde{y} \\frac{s_n \\ }{s_n + \\beta^*} + \\frac{\\beta^* }{s_n + \\beta^*}\\mu^*\n",
    "$$\n",
    "and precision\n",
    "$$\n",
    "\\Lambda_{0}[s_n + \\beta^*] \n",
    "$$\n",
    "\n",
    "The rest of the above quantity can be rearranged into a Wishart distribution:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "&\\log \\det \\Lambda_{0} \n",
    "\\frac{ \\sum_{i=1}^n  (1-p_i)  + \\nu-2-1 }{2}\n",
    "-.5 \\left\\{ \n",
    "\\sum_{i=1}^n  (1-p_i)y_i^\\intercal \\Lambda_0 y_i \n",
    "+ \\beta^*\\mu^* \\Lambda_0 \\mu^* \\right\\} \n",
    "- .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_0)\n",
    "+ c \\\\ \\\\\n",
    "&=\n",
    "\\log \\det \\Lambda_{0} \n",
    "\\frac{ \\sum_{i=1}^n  (1-p_i)  + \\nu-2-1 }{2}\n",
    "-.5 \\text{tr}\\left\\{ \n",
    "\\sum_{i=1}^n  (1-p_i)y_i^\\intercal \\Lambda_0 y_i \n",
    "+ \\beta^*(\\mu^* )^\\intercal \\Lambda_0 \\mu^* \\right\\} \n",
    "- .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_0)\n",
    "+ c \\\\\n",
    "&=\n",
    "\\log \\det \\Lambda_{0} \n",
    "\\frac{ \\sum_{i=1}^n  (1-p_i)  + \\nu-2-1 }{2}\n",
    "-.5 \\left\\{ \n",
    "\\text{tr} \\left[ \\sum_{i=1}^n  (1-p_i)y_i^\\intercal \\Lambda_0 y_i \\right]\n",
    "+ \\text{tr}\\left[\\beta^*(\\mu^* )^\\intercal \\Lambda_0 \\mu^* \\right] +\n",
    "\\text{tr}(\\mathbf{W}^{-1} \\Lambda_0)\n",
    "\\right\\} \n",
    "+ c \\\\ \n",
    "&=\n",
    "\\log \\det \\Lambda_{0} \n",
    "\\frac{ \\sum_{i=1}^n  (1-p_i)  + \\nu-2-1 }{2}\n",
    "-.5 \\left\\{ \n",
    "\\text{tr} \\left[ \\sum_{i=1}^n  (1-p_i)  y_iy_i^\\intercal \\Lambda_0\\right]\n",
    "+ \\text{tr}\\left[\\beta^* \\mu^*(\\mu^* )^\\intercal \\Lambda_0\\right] + \n",
    "\\text{tr}(\\mathbf{W}^{-1} \\Lambda_0)\n",
    "\\right\\} \n",
    "+ c \\\\\n",
    "&=\n",
    "\\log \\det \\Lambda_{0} \n",
    "\\frac{ \\sum_{i=1}^n  (1-p_i)  + \\nu-2-1 }{2}\n",
    "-.5 \\text{tr} \\left\\{ \\left[\n",
    " \\sum_{i=1}^n  (1-p_i)  y_iy_i^\\intercal\n",
    "+ \\beta^* \\mu^*(\\mu^* )^\\intercal \n",
    "+ \\mathbf{W}^{-1} \n",
    "\\right]\\Lambda_0\n",
    "\\right\\} \n",
    "+ c \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This is a Wishart with $\\sum_{i=1}^n  (1-p_i)  + \\nu$ degrees of freedom and a scale matrix of \n",
    "$$\n",
    " \\sum_{i=1}^n  (1-p_i)  y_iy_i^\\intercal\n",
    "+ \\beta^* \\mu^*(\\mu^* )^\\intercal \n",
    "+ \\mathbf{W}^{-1} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8050b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mu_lambda_zero(old_g_hyper_params, ydata, prior_hyperparams):\n",
    "    \"\"\"this function takes and old g approx posterior\n",
    "    and then returns a new g approx posterior\n",
    "    the new one will have updated alpha and betas\"\"\"\n",
    "    # unpack prior hyperparameters and g hyperparameters\n",
    "    p_array, alpha, beta, m_zero, m_one, c_zero, c_one, H_zero, H_one, tau_zero, tau_one = old_g_hyper_params\n",
    "    mu_star, beta_star, W, nu, a, b = prior_hyperparams\n",
    "\n",
    "    # update mu_zero | lambda_zero distribution first\n",
    "    sn = np.sum((1.0 - p_array))\n",
    "    ncols = ydata.shape[1]\n",
    "    y_tilde = np.zeros(ncols)\n",
    "    ws_outer_prods = np.zeros((ncols,ncols))\n",
    "    for idx,yi in ydata.iterrows():\n",
    "        one_minus_p = 1.0 - p_array[idx]\n",
    "        y_tilde += yi.values*one_minus_p\n",
    "        ws_outer_prods += one_minus_p * (yi.values @ np.transpose(yi.values))\n",
    "    y_tilde /= sn\n",
    "    new_mean = y_tilde * sn / (sn + beta_star) + mu_star * beta_star / (sn + beta_star)\n",
    "    \n",
    "    # update Lambda_zero distribution\n",
    "    new_df = sn + nu\n",
    "    new_scale_mat  = np.linalg.inv(W) + beta_star * mu_star @ np.transpose(mu_star) + ws_outer_prods\n",
    "    \n",
    "    # return everything\n",
    "    return p_array, alpha, beta, new_mean, m_one, sn + beta_star, c_one, new_scale_mat, H_one, new_df, tau_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22602e90",
   "metadata": {},
   "source": [
    "### 4. changing $\\mu_1$ and $\\Lambda_1$'s distribution at each iteration\n",
    "\n",
    "Fortunately, this work is very similar to the above step:\n",
    "\n",
    "Here we take the expectation of \n",
    "\n",
    "\\begin{align*}\n",
    "\\log p(\\mathbf{y}, \\mathbf{x}, \\theta ) \n",
    "&= c + \\sum_{i=1}^n  \\mathbb{I}(x_i = 1)\\left[ \\log p(y_i \\mid x_i=1, \\theta) + \\log p(x_i=1 \\mid \\theta)\\right]  + \\log p(\\mu_1 \\mid \\Lambda_1) + \\log p(\\Lambda_1) \\\\\n",
    "&= c + \\sum_{i=1}^n  \\mathbb{I}(x_i = 1)\\log p(y_i \\mid x_i=1, \\theta)  + \\log p(\\mu_1 \\mid \\Lambda_1) + \\log p(\\Lambda_1)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "with respect to \n",
    "$$\n",
    "g(\\mathbf{x})g(\\pi)g(\\mu_0 \\mid \\Lambda_0)g(\\Lambda_0).\n",
    "$$ \n",
    "\n",
    "Again, the expectation is very simple:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n  p_i \\left[  .5 \\log \\det \\Lambda_{1} - .5 (y_i - \\mu_{1})^\\intercal \\Lambda_{1}(y_i - \\mu_{1})]  \\right] \n",
    "+\\log p(\\mu_1 \\mid \\Lambda_1) + \\log p(\\Lambda_1)+ c\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435643d3",
   "metadata": {},
   "source": [
    "But the rearrangement into a Normal-Wishart distribution can be tedious. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "&= .5 \\log \\det \\Lambda_{1} \n",
    "\\left\\{ \\sum_{i=1}^n  p_i + 1 + \\nu -2-1 \\right\\}\n",
    "- .5 \\sum_{i=1}^n  p_i (\\mu_{1} - y_i )^\\intercal \\Lambda_{1}(\\mu_{1}- y_i )   \\\\\n",
    "&\\hspace{10mm} \n",
    "- .5 \\beta^*(\\mu_1 - \\mu^*)^\\intercal\\Lambda_1(\\mu_1 - \\mu^*) - .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_1)\n",
    "+ c \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The second chunk is tedious to simplify again\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n  p_i (\\mu_{1} - y_i )^\\intercal \\Lambda_{1}(\\mu_{1}- y_i )\n",
    "&=\n",
    "\\mu_{1}^\\intercal r_n \\Lambda_{1}\\mu_{1} \n",
    "- 2 \\mu_1^\\intercal r_n \\Lambda_1\\ \\check{y} \n",
    "+\\sum_{i=1}^n  p_i y_i^\\intercal \\Lambda_1 y_i\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where $r_n = \\sum_{i=1}^n  p_i $ and $\\check{y} = r_n^{-1}\\sum_i p_i y_i$ is another weighted average of the observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe39488",
   "metadata": {},
   "source": [
    "The third chunk is again not bad, so let's get the final formula now\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "&\\mathbb{E}_{-(\\mu_1, \\Lambda_1)}\n",
    "\\log p(\\mathbf{y}, \\mathbf{x}, \\theta ) \n",
    "\\\\\n",
    "&=.5 \\log \\det \\Lambda_{1} \n",
    "\\left\\{ \\sum_{i=1}^n  p_i + 1 + \\nu -2-1 \\right\\}\n",
    "- .5 \\left\\{ \\mu_{1}^\\intercal r_n \\Lambda_{1}\\mu_{1} \n",
    "- 2 \\mu_1^\\intercal r_n \\Lambda_0\\ \\check{y} \n",
    "+\\sum_{i=1}^n  p_i y_i^\\intercal \\Lambda_1 y_i\\right\\}   \\\\\n",
    "&\\hspace{10mm} \n",
    "- .5 \\left\\{ \\beta^*\\mu_1^\\intercal \\Lambda_1 \\mu_1 - 2 \\beta^*\\mu_1^\\intercal\\Lambda_1 \\mu^* + \\beta^*\\mu^* \\Lambda_1 \\mu^* \\right\\}\n",
    "- .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_1)\n",
    "+ c \\\\ \\\\\n",
    "&=.5 \\log \\det \\Lambda_{1} \n",
    "\\left\\{ \\sum_{i=1}^n  p_i + 1 + \\nu-2-1 \\right\\}\n",
    "- .5 \n",
    "\\left(\\mu_{1} -  \\left[\\Lambda_{1}[r_n + \\beta^*] \\right]^{-1} \\left[r_n \\Lambda_1\\ \\check{y}  +  \\beta^* \\Lambda_1 \\mu^*\\right] \\right)^\\intercal\n",
    "\\left[\\Lambda_{1}[r_n + \\beta^*] \\right]\n",
    "\\left(\\mu_{1} -  \\left[\\Lambda_{1}[r_n + \\beta^*] \\right]^{-1} \\left[r_n \\Lambda_1\\ \\check{y}  +  \\beta^* \\Lambda_1 \\mu^*\\right] \\right)\n",
    " \\\\\n",
    "&\\hspace{10mm} \n",
    "-.5 \\left\\{ \n",
    "\\sum_{i=1}^n  p_i y_i^\\intercal \\Lambda_1 y_i \n",
    "+ \\beta^*\\mu^* \\Lambda_1 \\mu^* \\right\\} \n",
    "- .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_1)\n",
    "+ c \\\\\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7cf909",
   "metadata": {},
   "source": [
    "We recognize the quadratic form in the middle, which means $g(\\mu_0 \\mid \\Lambda_0)$ can be updated to a normal distribution with mean \n",
    "$$\n",
    "\\check{y} \\frac{r_n \\ }{r_n + \\beta^*} + \\frac{\\beta^* }{r_n + \\beta^*}\\mu^*\n",
    "$$\n",
    "and precision\n",
    "$$\n",
    "\\Lambda_{1}[r_n + \\beta^*] \n",
    "$$\n",
    "\n",
    "The rest of the above quantity can be rearranged into a Wishart distribution:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "&= \\log \\det \\Lambda_{1} \n",
    "\\frac{\\sum_{i=1}^n  p_i + \\nu-2-1}{2}\n",
    "-.5 \\left\\{ \n",
    "\\sum_{i=1}^n  p_i y_i^\\intercal \\Lambda_1 y_i \n",
    "+ \\beta^*\\mu^* \\Lambda_1 \\mu^* \\right\\} \n",
    "- .5\\text{tr}(\\mathbf{W}^{-1} \\Lambda_1)\n",
    "+ c \\\\\n",
    "&=\n",
    "\\log \\det \\Lambda_{1} \n",
    "\\frac{ \\sum_{i=1}^n  p_i  + \\nu-2-1 }{2}\n",
    "-.5 \\text{tr} \\left\\{ \\left[\n",
    " \\sum_{i=1}^n  p_i  y_iy_i^\\intercal\n",
    "+ \\beta^* \\mu^*(\\mu^* )^\\intercal \n",
    "+ \\mathbf{W}^{-1} \n",
    "\\right]\\Lambda_1\n",
    "\\right\\} \n",
    "+ c \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This is a Wishart with $\\sum_{i=1}^n  p_i  + \\nu$ degrees of freedom and a scale matrix of \n",
    "$$\n",
    " \\sum_{i=1}^n  p_i y_iy_i^\\intercal\n",
    "+ \\beta^* \\mu^*(\\mu^* )^\\intercal \n",
    "+ \\mathbf{W}^{-1} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a79894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mu_lambda_one(old_g_hyper_params, ydata, prior_hyperparams):\n",
    "    \"\"\"this function takes and old g approx posterior\n",
    "    and then returns a new g approx posterior\n",
    "    the new one will have updated alpha and betas\"\"\"\n",
    "    # unpack prior hyperparameters and g hyperparameters\n",
    "    p_array, alpha, beta, m_zero, m_one, c_zero, c_one, H_zero, H_one, tau_zero, tau_one = old_g_hyper_params\n",
    "    mu_star, beta_star, W, nu, a, b = prior_hyperparams\n",
    "\n",
    "    # update mu_zero | lambda_zero distribution first\n",
    "    rn = np.sum(p_array)\n",
    "    ncols = ydata.shape[1]\n",
    "    y_check = np.zeros(ncols)\n",
    "    ws_outer_prods = np.zeros((ncols,ncols))\n",
    "    for idx,yi in ydata.iterrows():\n",
    "        y_check += yi.values*p_array[idx]\n",
    "        ws_outer_prods += p_array[idx] * (yi.values @ np.transpose(yi.values))\n",
    "    y_check /= rn\n",
    "    new_mean = y_check * rn / (rn + beta_star) + mu_star * beta_star / (rn + beta_star)\n",
    "    \n",
    "    # update Lambda_zero distribution\n",
    "    new_df = rn + nu\n",
    "    new_scale_mat  = np.linalg.inv(W) + beta_star * mu_star @ np.transpose(mu_star) + ws_outer_prods\n",
    "    \n",
    "    # return everything\n",
    "    return p_array, alpha, beta, m_zero, new_mean, c_zero, rn+beta_star, H_zero, new_scale_mat, tau_zero, new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dec19a",
   "metadata": {},
   "source": [
    "## All the code in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccac88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize prior hyperparameters\n",
    "mu_star = np.zeros(2) \n",
    "beta_star = 1\n",
    "W = np.eye(2) \n",
    "nu = 3 \n",
    "a = b = 2\n",
    "prior = mu_star, beta_star, W, nu, a, b\n",
    "\n",
    "\n",
    "# initialize g hyperparameters\n",
    "p_array = np.repeat(.5,data.shape[0])\n",
    "alpha = beta=2\n",
    "m_zero = np.array([2,50])\n",
    "m_one = np.array([4.5, 80])\n",
    "c_zero = c_one = 1\n",
    "H_zero = H_one = np.eye(2)\n",
    "tau_zero = tau_one =  10\n",
    "g = p_array, alpha, beta, m_zero, m_one, c_zero, c_one, H_zero, H_one, tau_zero, tau_one \n",
    "\n",
    "\n",
    "# refine g in four steps\n",
    "for i in range(10):\n",
    "    g = update_x(g, data, prior)\n",
    "    g = update_pi(g, data, prior)\n",
    "    g = update_mu_lambda_zero(g, data, prior)\n",
    "    g = update_mu_lambda_one(g, data, prior)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "677c3900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]),\n",
       " 2,\n",
       " 2,\n",
       " array([ 2, 50]),\n",
       " array([ 4.5, 80. ]),\n",
       " 1,\n",
       " 1,\n",
       " array([[1., 0.],\n",
       "        [0., 1.]]),\n",
       " array([[1., 0.],\n",
       "        [0., 1.]]),\n",
       " 10,\n",
       " 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before \n",
    "p_array, alpha, beta, m_zero, m_one, c_zero, c_one, H_zero, H_one, tau_zero, tau_one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b68c754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
       "        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.]),\n",
       " 162.0,\n",
       " 114.0,\n",
       " array([ 2.24410619, 55.89380531]),\n",
       " array([ 4.31734783, 80.54658385]),\n",
       " 113.0,\n",
       " 161.0,\n",
       " array([[362768.490486, 362767.490486],\n",
       "        [362767.490486, 362768.490486]]),\n",
       " array([[1058161.328489, 1058160.328489],\n",
       "        [1058160.328489, 1058161.328489]]),\n",
       " 115.0,\n",
       " 163.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8953afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden cells below here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f251f9",
   "metadata": {},
   "source": [
    "<!-- # Example 1: the EM algorithm\n",
    "\n",
    "...time permitting... -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2832f",
   "metadata": {},
   "source": [
    "<!-- ### Step 1/2: Find $Q(\\theta \\mid \\theta^{k-1})$\n",
    "\n",
    "We need the conditional posterior of the hidden stuff given $\\theta$:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{x} \\mid \\theta, \\mathbf{y}) \n",
    "&\\propto  p(\\mathbf{y} \\mid \\mathbf{x}, \\theta)p(\\mathbf{x} \\mid \\theta)p(\\theta) \\\\\n",
    "&\\propto  p(\\mathbf{y} \\mid \\mathbf{x}, \\theta)p(\\mathbf{x} \\mid \\theta) \\\\\n",
    "&= \\prod_{i=1}^n p(y_i \\mid x_i, \\theta)p(x_i \\mid \\theta) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "So $p(\\mathbf{x} \\mid \\theta, \\mathbf{y}) = \\prod_{i=1}^n p(x_i \\mid y_i, \\theta)$ and we can work on individual pieces for individual $i$. For example, \n",
    "\n",
    "\n",
    "$$\n",
    "p(x_i = 1  \\mid \\theta, y_i) = \\frac{ \\pi \\text{Normal}(y_i \\mid \\mu_1, \\Sigma_1) }{(1-\\pi) \\text{Normal}(y_i \\mid \\mu_0, \\Sigma_0) + \\pi \\text{Normal}(y_i \\mid \\mu_1, \\Sigma_1)}\n",
    "$$\n",
    "\n",
    "In other words, $x_i \\mid \\theta, y_i$ is $\\text{Bernoulli}$ with the above probability, so we can use formulas for that random variable for the expected values and variances, etc.\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d6fe9",
   "metadata": {},
   "source": [
    "<!-- \n",
    "Now that we have the conditional posterior $p(\\mathbf{x} \\mid \\theta, \\mathbf{y})$, we want to take the expectation of the log of the joint density: $\\log p(\\theta, \\mathbf{x} \\mid \\mathbf{y})$. This expectation is taken with respect to the above distribution with the old parameter value plugged in $\\theta^{k-1}$\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "Q(\\theta \\mid \\theta^{k-1}) \n",
    "&=\\mathbf{E}_{\\mathbf{x} \\mid \\theta^{k-1}, \\mathbf{y}} \\left[ \\log p(\\theta, \\mathbf{x} \\mid \\mathbf{y}) \\right]\\\\\n",
    "&= \\mathbf{E}_{\\mathbf{x} \\mid \\theta^{k-1}, \\mathbf{y}} \\left[ \\sum_{i=1}^n \\sum_{j=0}^1  \\mathbb{I}(x_i = j) \\left[ \\log p(y_i \\mid x_i=j, \\theta) + \\log p(x_i=j \\mid \\theta)\\right] \\right] + c \\\\\n",
    "&= \\sum_{i=1}^n \\mathbf{E}_{\\mathbf{x} \\mid \\theta^{k-1}, \\mathbf{y}} \\left\\{\\sum_{j=0}^1   \\mathbb{I}(x_i = j) \\left[ \\log p(y_i \\mid x_i=j, \\theta) + \\log p(x_i=j \\mid \\theta)\\right] \\right\\} + c \\\\\n",
    "&= \\sum_{i=1}^n \\mathbf{E}_{x_i \\mid \\theta^{k-1}, y_i} \\left\\{\\sum_{j=0}^1   \\mathbb{I}(x_i = j) \\left[ \\log p(y_i \\mid x_i=j, \\theta) + \\log p(x_i=j \\mid \\theta)\\right] \\right\\} + c \\\\\n",
    "&= \\sum_{i=1}^n \\sum_{j=0}^1 p(x_i=j \\mid \\theta^{k-1}, y_i) \\left[ \\log p(y_i \\mid x_i=j, \\theta) + \\log p(x_i=j \\mid \\theta) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff19e3d",
   "metadata": {},
   "source": [
    "<!-- ### Step 2/2: Find $\\theta$ that maximizes $Q(\\theta \\mid \\theta^{k-1})$\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta Q(\\theta \\mid \\theta^{k-1}) \\overset{\\text{set}}{=} \\mathbf{0}\n",
    "$$\n",
    "\n",
    "defines a system of equations. At every iteration $k$, you would set $\\theta^k$ equal to the solution of that system of equations.\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1374042",
   "metadata": {},
   "source": [
    "<!-- ### The marginal posterior: $ p(\\theta \\mid \\mathbf{y}) $\n",
    "\n",
    "Sum out unobserved x from the above expression to get the marginal posterior:\n",
    "$$\n",
    "p(\\theta \\mid \\mathbf{y}) = \n",
    "\\sum_{x_1=0}^1 \\cdots \\sum_{x_n=0}^1 p(\\theta, \\mathbf{x} \\mid \\mathbf{y})\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\theta \\mid \\mathbf{y})\n",
    "&\\propto\n",
    "p(\\theta)\\sum_{x_1} \\cdots \\sum_{x_n} p(\\mathbf{y} \\mid \\mathbf{x}, \\theta) p(\\mathbf{x} \\mid \\theta) \\\\\n",
    "&= \n",
    "p(\\theta)\\sum_{x_1} \\cdots \\sum_{x_n} \\prod_{i=1}^n p(y_i \\mid x_i, \\theta) p(x_i \\mid \\theta) \\\\\n",
    "&= \n",
    "p(\\theta)\\prod_{i=1}^n \\sum_{x_i} p(y_i \\mid x_i, \\theta) p(x_i \\mid \\theta) \\\\\n",
    "&= \n",
    "p(\\theta)\\prod_{i=1}^n \\{ p(y_i \\mid x_i=0, \\theta) (1-\\pi) +  p(y_i \\mid x_i=1, \\theta) \\pi \\}\n",
    "\\end{align*}\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\log p(\\theta \\mid \\mathbf{y}) = \\log p(\\theta) + \\sum_{i=1}^n \\log \\{ p(y_i \\mid x_i=0, \\theta) (1-\\pi) +  p(y_i \\mid x_i=1, \\theta) \\pi \\}\n",
    "$$\n",
    "\n",
    "and you can write a function that evaluates that function, and then pass it in to some function in [`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/optimize.html). -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
